# Decentralized adaptive work package (DAWP) learning

the implementation of DAWP learning

---
### The description of all files
1. dawp_main.py: the main function for all experiments;
2. dawp_pretrain.py: this script includes different pretrain models;split video data into image data, then use lightweight pretrain models to have smaller and high quality inputs;
3. experimental_results_analysis.py: this script analyzes experimental results and plot;
4. utils.py: pre-processes all data set;the needed extra functions
5. local_learning.py: an other method;
6. federated_main.py: the main function for an other method;
7. cga_main.py: the main function for an other method.
---

### Preparations

Two virtual machines are employed in the experimental environment, running on a Linux Ubuntu 20.04 system, to execute the training and evaluation process. The computer specifications and development package details for this experiment are provided below:
1. Hardware: 20 cores, 64 GB of RAM, 256 GB SSD, and 2 TB HDD
2. Network: Each node can accommodate up to three open ports
3. Architecture: AMD64
4. Hosting Platform for containers: Docker 18.01.0
5. CPU: Intel R XeR (R) E5-2640 v4@ 2.40 GHz (20 CPUs)
6. GPU: NVIDIA GeForce GTX 1080
7. Deep learning framework: Python 3.73, Keras 2.3.1, TensorFlow 1.15.0

### Datasets
email joy2023@connect.hku.hk for datasets.
1. **climb** whether climb from the bottom to the top of the ladder
2. **wear_helmet** whether put the helmet on the head
3. **smoke** whether smoke cigarettes in the working area
4. **dangerous_actions** new tasks that is dangerous actions in the working area or not

## Get Started

### Parameters Description in dawp_main.py for Running all experiments
1. dataset: the task for running experiments;
2. sync_interval: specifies the number of batches of local training to be performed between two sync rounds. If adaptive sync enabled, this is the frequency to be used at the start;
3. num_peers: specifies the number of peers required during each synchronization round for Swarm Learning to proceed further;
4. learning_rate: determines the step size at each iteration while moving toward a minimum of a loss function;
5. batch_size: determines the number of samples utilized in one iteration;
6. num_users: the num of clients in each experiment;

### Example Synthetic Experiment

```
python dawp_main.py
```
## Acknowledgements

This project is based on the following open-source projects for secondary development, and we would like to express our gratitude to the related projects and research and development personnel.


## Disclaimer

**The resources related to this project are for academic research purposes only and are strictly prohibited for commercial use.** When using parts involving third-party code, please strictly follow the corresponding open-source agreements. The content generated by the model is affected by factors such as model calculation, randomness, and quantization accuracy loss. This project cannot guarantee its accuracy. For any content output by the model, this project does not assume any legal responsibility and does not assume responsibility for any losses that may result from the use of related resources and output results.

This project is initiated and maintained by individuals and collaborators in their spare time, so we cannot guarantee a timely response to resolving relevant issues.

## Feedback

If you have any questions, please submit them in GitHub Issues.

- Before submitting a question, please check if the FAQ can solve the problem and consult past issues to see if they can help.
- Please use our dedicated issue template for submitting.
- Duplicate and unrelated issues will be handled by [stable-bot](https://github.com/marketplace/stale); please understand.
- Raise questions politely and help build a harmonious discussion community.